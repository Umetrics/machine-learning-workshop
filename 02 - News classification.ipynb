{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Category Classification\n",
    "\n",
    "In this notebook we will work with whe 20 newsgroups dataset that comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "First we load the data. This may take a couple of minutes and requires internet access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bunch = fetch_20newsgroups(subset='train', remove=('footers', ))\n",
    "test_bunch = fetch_20newsgroups(subset='test', remove=('footers', ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In machine learning we need to work with some kind of table representation. In this example the raw data is pieces of text, and therefore needs feature engineering to find a numerical representation we can feed to our machine learning algorithms.\n",
    "\n",
    "In this notebook we will use a very simple, but effective, methods called a _bag-of-words_. The bag-of-words is based on counting how often words occur. The procedure looks like:\n",
    "\n",
    "1. Preprocess raw text, for instance transforming the documents into lower case.\n",
    "2. Tokenize preprocessed texts. In this step we split the text into \"tokens\" that constitute the pieces that we want to work with. This is typically words, or combinations of subsequent words, in cases of text categorization.\n",
    "3. Build a vocabulary of tokens by filtering, for instance removing numericals, non-informative words such as \"and\"/\"or\"/... or some domain specific filtering such as using only names.\n",
    "4. Based on the vocabulary, describe each document as a vector of counts of each word in the vocabulary. This collection of vectors is our bag-of-words.\n",
    "\n",
    "First we take a look at what the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bunch.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use scikit-learn [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class to build our bag-of-words. The `CountVectorizer` class implements the whole procedure descibed above, and allows plenty of customization.\n",
    "\n",
    "Fitting our vectorizer is a one-liner. The `min_df` keyword specifies how large share of the documents a token must occur in in order to be included in the vocabulary. We set a 5 \\% threshold to get a more manageable amount of words for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=0.05).fit(train_bunch.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how the preprocessing works, we can separate the preprocessing and tokenization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = count_vectorizer.build_preprocessor()\n",
    "tokenizer = count_vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default preprocessing is simply using lower case text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from: lerxst@wam.umd.edu (where's my thing)\\nsubject: what car is this!?\\nnntp-posting-host: rac3.wam.umd.edu\\norganization: university of maryland, college park\\nlines: 15\\n\\n i was wondering if anyone out there could enlighten me on this car i saw\\nthe other day. it was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. it was called a bricklin. the doors were really small. in addition,\\nthe front bumper was separate from the rest of the body. this is \\nall i know. if anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_text = preprocessor(train_bunch.data[0])\n",
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default tokenization involves simple word splitting based on whitespace and punctuation. This often works well, but will split compound words such as \"New York\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from',\n",
       " 'lerxst',\n",
       " 'wam',\n",
       " 'umd',\n",
       " 'edu',\n",
       " 'where',\n",
       " 'my',\n",
       " 'thing',\n",
       " 'subject',\n",
       " 'what',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " 'nntp',\n",
       " 'posting',\n",
       " 'host',\n",
       " 'rac3',\n",
       " 'wam',\n",
       " 'umd',\n",
       " 'edu',\n",
       " 'organization',\n",
       " 'university',\n",
       " 'of',\n",
       " 'maryland',\n",
       " 'college',\n",
       " 'park',\n",
       " 'lines',\n",
       " '15',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " 'it',\n",
       " 'was',\n",
       " 'door',\n",
       " 'sports',\n",
       " 'car',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s',\n",
       " 'early',\n",
       " '70s',\n",
       " 'it',\n",
       " 'was',\n",
       " 'called',\n",
       " 'bricklin',\n",
       " 'the',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'know',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'model',\n",
       " 'name',\n",
       " 'engine',\n",
       " 'specs',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " 'history',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " 'please',\n",
       " 'mail']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(preprocessed_text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete tokenizer returns the bag-of-words matrix in sparse format by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 1, 0, 0,\n",
       "        0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2,\n",
       "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform(train_bunch.data[:1]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has quite many observations, 11314 in training set, so we will use a fixed train-/validation-data split. We will use 20 \\% of the training set as validation-set and do a selection stratified on the class so we validate on equal amounts of documents from each class.\n",
    "\n",
    "Since we have a time dimension, we may select the validation set to be the latest articles in each class. But since the test set is selected based on date, we simply choose randomly from each class for validation set. It will probably suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_bunch.data, train_bunch.target, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=42,\n",
    "                                                  stratify=train_bunch.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things easy, we will use the `Pipeline` class again. This time we will use a [Naive Bayes classifier](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy: 84.401 %\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "val_accuracy = metrics.accuracy_score(val_y, pipeline.predict(val_x))\n",
    "\n",
    "print(f'Validation set accuracy: {val_accuracy * 100:.3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 84 \\% on the first attempt is not too shabby.\n",
    "\n",
    "### Optimization\n",
    "\n",
    "In the example above we used default settings for both preprocessing and model fitting. This is probably not optimal and we would like to optimize the results.\n",
    "\n",
    "We will use something called grid-search, which is a brute-force algorithm for optimizing parameters. The user specifies a parameter grid, and a model will be fitted for each point in the grid. The point with the best validation-set performance is the model we then choose to continue with. Grid search is implemented in the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class and described in the [user guide](https://scikit-learn.org/stable/modules/grid_search.html#grid-search).\n",
    "\n",
    "First we unfortunately have to redo our validation split since `GridSearchCV` assumes k-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = StratifiedKFold(n_splits=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify our parameter grid. We optimize two parameters, namely `min_df` of the `CountVectorizer` and `alpha` of `MultinomialNB`. We can of course optimize more parameters, but the computational demands will increase.\n",
    "\n",
    "This fitting may already take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n",
      "Best validation accuracy: 89.110836 %\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'vectorizer__min_df': [0, 0.01, 0.05],\n",
    "    'model__alpha': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=folder)\n",
    "\n",
    "%time grid_search.fit(train_bunch.data, train_bunch.target)  # We time the fitting using %time jupyter magic.\n",
    "\n",
    "print(f'Best validation accuracy: {grid_search.best_score_ * 100:3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to squeeze out another 5 percentages on the validation set. \n",
    "\n",
    "We decide to go with this optimized model and report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-set accuracy: 81.213489 %\n"
     ]
    }
   ],
   "source": [
    "test_predicted_target = grid_search.best_estimator_.predict(test_bunch.data)\n",
    "\n",
    "test_accuracy = metrics.accuracy_score(test_bunch.target, test_predicted_target)\n",
    "\n",
    "print(f'Test-set accuracy: {test_accuracy * 100:3f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Boston Housing dataset, the test-set accuracy is lower than the validation one. But this is what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your attempts\n",
    "\n",
    "Feel free to experiment with the data available.\n",
    "\n",
    "Suggestions:\n",
    "* Try another type of model. For instance a [Support Vector Classifer](https://scikit-learn.org/stable/modules/svm.html#svm-classification) or [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "* Try to add another type of pre-processing after the `CountVectorizer`. For instance a [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) that re-weighs the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
